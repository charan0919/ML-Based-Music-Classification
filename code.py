# -*- coding: utf-8 -*-
"""Final IML_project_33 .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MOc2_1UTfNvx6ZTRtNsdGdcQFxO2_Bxh
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import joblib
import sys
import os
import pickle
import librosa
import librosa.display
import IPython.display as ipd
from IPython.display import Audio
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
import tensorflow as tf
from tensorflow import keras
import soundfile as sf
import numpy as np
import pandas as pd
import scipy.io.wavfile as wav
import seaborn as sns
import os
import warnings
warnings.filterwarnings("ignore")

"""**Database Generation from Audio Files**"""

# Extracting the required features from the audio files
def extract_features(audio, sample_rate):
    features = []
    # Chroma Features
    chroma_stft = librosa.feature.chroma_stft(y=audio, sr=sample_rate)
    chroma_stft_mean = np.mean(chroma_stft).astype(np.float32)
    chroma_stft_var = np.var(chroma_stft).astype(np.float32)
    features.extend([chroma_stft_mean])
    features.extend([chroma_stft_var])

    # RMS
    rms = librosa.feature.rms(y=audio)
    rms_mean = np.mean(rms).astype(np.float32)
    rms_var = np.var(rms).astype(np.float32)
    features.extend([rms_mean])
    features.extend([rms_var])

    # Spectral Centroid
    spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sample_rate)
    spectral_centroid_mean = np.mean(spectral_centroid).astype(np.float32)
    spectral_centroid_var = np.var(spectral_centroid).astype(np.float32)
    features.extend([spectral_centroid_mean])
    features.extend([spectral_centroid_var])

    # Spectral Bandwidth
    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sample_rate)
    spectral_bandwidth_mean = np.mean(spectral_bandwidth).astype(np.float32)
    spectral_bandwidth_var = np.var(spectral_bandwidth).astype(np.float32)
    features.extend([spectral_bandwidth_mean])
    features.extend([spectral_bandwidth_var])

    # Rolloff
    rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sample_rate)
    rolloff_mean = np.mean(rolloff).astype(np.float32)
    rolloff_var = np.var(rolloff).astype(np.float32)
    features.extend([rolloff_mean])
    features.extend([rolloff_var])

    # Zero Crossing Rate
    zero_crossing_rate = librosa.feature.zero_crossing_rate(y=audio)
    zero_crossing_rate_mean = np.mean(zero_crossing_rate).astype(np.float32)
    zero_crossing_rate_var = np.var(zero_crossing_rate).astype(np.float32)
    features.extend([zero_crossing_rate_mean])
    features.extend([zero_crossing_rate_var])

    # Harmony and Perceptr
    harmony, perceptr = librosa.effects.hpss(audio)
    harmony_mean = np.mean(harmony).astype(np.float32)
    harmony_var = np.var(harmony).astype(np.float32)
    perceptr_mean = np.mean(perceptr).astype(np.float32)
    perceptr_var = np.var(perceptr).astype(np.float32)
    features.extend([harmony_mean])
    features.extend([harmony_var])
    features.extend([perceptr_mean])
    features.extend([perceptr_var])

    # Tempo
    tempo = librosa.feature.rhythm.tempo(y=audio, sr=sample_rate)[0].astype(np.float32)
    features.extend([tempo])

    # MFCCs
    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=20)
    mfcc_means = np.mean(mfccs, axis=1).astype(np.float32)
    mfcc_vars = np.var(mfccs, axis=1).astype(np.float32)

    features.extend(mfcc_means)
    features.extend(mfcc_vars)

    return features

import os
import librosa
import numpy as np
import pandas as pd

# Path to your dataset in Google Drive
path = "/content/drive/MyDrive/archive/Data/genres_original"

print("Path to dataset files:", path)
dataset = []
y = []

def extract_features(data, sample_rate):
    # Define your feature extraction method here
    # Example: returning MFCCs as features
    mfccs = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=13)
    return np.mean(mfccs.T, axis=0)

# Parsing the Folders
for i, folder in enumerate(os.listdir(path)):
    folder_path = os.path.join(path, folder)

    # Ensure the item is a directory
    if not os.path.isdir(folder_path):
        print(f"Skipping non-directory item: {folder}")
        continue

    for j, file in enumerate(os.listdir(folder_path)):
        print(i, j, file)

        # Read the Audio File
        try:
            file_path = os.path.join(folder_path, file)
            data, sample_rate = librosa.load(file_path, sr=22050)
            data = librosa.effects.trim(data)[0]
        except Exception as e:
            print(f"File {file} is corrupt or cannot be loaded: {e}")
            continue

        # Trims the 30-second file into 3-second intervals
        gap = sample_rate * 3
        tempo = librosa.beat.beat_track(y=data, sr=sample_rate)[0]
        for k in np.arange(0, len(data) - gap, gap):
            lower = int(k)
            upper = int(k + gap)

            row = extract_features(data[lower:upper], sample_rate)
            dataset.append(np.array(row))
            y.append(folder)

y = pd.DataFrame(y)

"""**EDA**"""

columns = ['chroma_stft_mean', 'chroma_stft_var', 'rms_mean',
       'rms_var', 'spectral_centroid_mean', 'spectral_centroid_var',
       'spectral_bandwidth_mean', 'spectral_bandwidth_var', 'rolloff_mean',
       'rolloff_var', 'zero_crossing_rate_mean', 'zero_crossing_rate_var',
       'harmony_mean', 'harmony_var', 'perceptr_mean', 'perceptr_var', 'tempo',
       'mfcc1_mean', 'mfcc1_var', 'mfcc2_mean', 'mfcc2_var', 'mfcc3_mean',
       'mfcc3_var', 'mfcc4_mean', 'mfcc4_var', 'mfcc5_mean', 'mfcc5_var',
       'mfcc6_mean', 'mfcc6_var', 'mfcc7_mean', 'mfcc7_var', 'mfcc8_mean',
       'mfcc8_var', 'mfcc9_mean', 'mfcc9_var', 'mfcc10_mean', 'mfcc10_var',
       'mfcc11_mean', 'mfcc11_var', 'mfcc12_mean', 'mfcc12_var', 'mfcc13_mean',
       'mfcc13_var', 'mfcc14_mean', 'mfcc14_var', 'mfcc15_mean', 'mfcc15_var',
       'mfcc16_mean', 'mfcc16_var', 'mfcc17_mean', 'mfcc17_var', 'mfcc18_mean',
       'mfcc18_var', 'mfcc19_mean', 'mfcc19_var', 'mfcc20_mean', 'mfcc20_var']

dataset = pd.DataFrame(dataset,columns=columns)
dataset['label'] = y
dataset.to_csv("/content/drive/MyDrive/archive/Data/genres_original/dataset.csv", index=False)

df = pd.read_csv('/content/drive/MyDrive/archive/Data/genres_original/dataset.csv')

columns_mean = [column for column in df.columns if 'mean' in column]
corr_mean = df[columns_mean].corr()

columns_var = [column for column in df.columns if 'var' in column]
corr_var = df[columns_var].corr()

plt.figure(figsize=(15, 24))

plt.subplot(2, 1, 1)
plt.title('Correlation Heatmap (Mean Columns)')
sns.heatmap(corr_mean, square=True, linewidths=.5, cmap="magma")

plt.subplot(2, 1, 2)
plt.title('Correlation Heatmap (Variance Columns)')
sns.heatmap(corr_var, square=True, linewidths=.5, cmap="inferno")
plt.savefig('/content/drive/MyDrive/archive/Data/genres_original/heatmap.png')

plt.tight_layout()
plt.show()

# Since tempo is alone without any mean and var hence we check for the outliers
x = dataset[["label", "tempo"]]
color = {"blues":"b","classical":"c","country":"o","disco":"r","hiphop":"t","jazz":"p","metal":"g","pop":"v","reggae":"y","rock":"w"}
f, ax = plt.subplots(figsize=(15, 10));
sns.boxplot(x = "label", y = "tempo", data = x, palette = "icefire");

plt.title('BPM Boxplot for Genres')
plt.savefig("/content/drive/MyDrive/archive/Data/genres_original/BPM Boxplot.jpg")

!pip install scikit-learn # Ensure scikit-learn is installed
import sklearn.decomposition
from sklearn.decomposition import PCA # Import PCA from sklearn.decomposition

pca = PCA()
pca.fit(df.drop(['label'], axis=1))

plt.figure(figsize=(6, 4))
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
plt.title('Elbow Method for Optimal Number of Components')
plt.grid()
plt.savefig("/content/drive/MyDrive/archive/Data/genres_original/Elbow_Variance.jpg")
plt.show()

from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA

y = dataset['label']
X = dataset.drop(columns='label')

scaler = MinMaxScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)

pca = PCA(n_components = 2)
df_pca = pd.DataFrame(pca.fit_transform(X), columns = ['Principal Component 1', 'Principal Component 2'])

final_df = pd.concat([df_pca, y], axis = 1)

plt.figure(figsize = (10,10))
sns.scatterplot(x = "Principal Component 1", y = "Principal Component 2", data = final_df, hue = "label", alpha = 0.7, s = 50);
plt.title('PCA on Genres')
plt.savefig("/content/drive/MyDrive/archive/Data/genres_original/PCA Scatter.jpg")

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

# Fit LDA model
lda = LDA(n_components=2)
X_lda = lda.fit_transform(X, y)

# Create a new dataframe with the LDA components and the original labels
final_df_lda = pd.DataFrame(X_lda, columns=['LD1', 'LD2'])
final_df_lda['label'] = y

# Plot the LDA graph
plt.figure(figsize=(10, 8))
sns.scatterplot(x='LD1', y='LD2', hue='label', data=final_df_lda, alpha=0.7, s=50)
plt.title('LDA on Genres')
plt.xlabel('Linear Discriminant 1')
plt.ylabel('Linear Discriminant 2')
plt.legend(loc='best')
plt.savefig("/content/drive/MyDrive/archive/Data/genres_original/LDA Scatter.jpg")
plt.show()

"""**Preprocessing the data**"""

import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/archive/Data/genres_original/dataset.csv')

df.head()

# Drop columns 'label' and 'Unnamed: 0' if they exist in the DataFrame
X = df.drop(columns=[col for col in ['label', 'Unnamed: 0'] if col in df.columns])

le = LabelEncoder()
y = le.fit_transform(df['label'])

X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=7)

X_train.head()

"""**Models**"""

from sklearn.model_selection import cross_val_score as cv
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import cross_val_score

"""**Logistic Regression**"""

model = LogisticRegression(max_iter=10000)

cv_scores = cross_val_score(model, X_train, y_train, cv=5)
print(f"CV Score for Logistic Regression: {max(cv_scores)}")

"""**Support Vector Machine**"""

model = SVC()

cv_scores = cross_val_score(model, X_train, y_train, cv=5)
print(f"CV Score for SVC: {max(cv_scores)}")

"""**KNN**"""

model = KNeighborsClassifier()

cv_scores = cross_val_score(model, X_train, y_train, cv=5)
print(f"CV Score for KNN: {max(cv_scores)}")

"""**Gaussian Naive Bayes**"""

model = GaussianNB()

cv_scores = cross_val_score(model, X_train, y_train, cv=5)
print(f"CV Score for Gaussian Naive Bayes: {max(cv_scores)}")

"""**Decision Tree**"""

model = DecisionTreeClassifier()

cv_scores = cross_val_score(model, X_train, y_train, cv=5)
print(f"CV Score for Decision Tree: {max(cv_scores)}")

"""**MLP**"""

model = MLPClassifier(hidden_layer_sizes=(54, 54,), max_iter=1000)

cv_scores = cross_val_score(model, X_train, y_train, cv=5)
print(f"CV Score for Neural Network: {max(cv_scores)}")

"""**Hyper parameter Tuningt**"""

joblib.dump(model, '/content/drive/MyDrive/archive/Data/genres_original.pkl')
joblib.dump(le, '/content/drive/MyDrive/archive/Data/genres_original.pkl')

print(accuracy_score(y_train,model.predict(X_train)))

"""**Function for predictiont**"""

model_file = "/content/drive/MyDrive/archive/Data/genres_original.pkl"
label_encoder = "/content/drive/MyDrive/archive/Data/genres_original.pkl"

def Genre_Classifier(path):
    audio_data , sample_rate = librosa.load(path)
    audio_data = librosa.effects.trim(audio_data)[0]

    features = []
    divided = sample_rate * 3
    for k in np.arange(0,len(audio_data)-divided, divided):
        lower = int(k)
        upper = int(k + gap)

        feature_row = extract_features(audio_data[lower:upper], sample_rate)
        features.append(np.array(feature_row))

    features = pd.DataFrame(features)

    classifier = joblib.load(model_file)
    result = classifier.predict(features)

    decode = joblib.load(label_encoder)
    return decode.inverse_transform([pd.DataFrame(result).mode()[0][0]])[0]

Genre_Classifier('/content/drive/MyDrive/archive/Data/genres_original/blues/blues.00001.wav')
